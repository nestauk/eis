{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web sources\n",
    "\n",
    "This is a sandbox to explore potential web indicator data collections for EIS.\n",
    "\n",
    "We will:\n",
    "\n",
    "* Create a summary table\n",
    "* Collect LinkedIn skills migration data\n",
    "* Explore options to query Google Big query about:\n",
    "  * GitHub\n",
    "  * Python downloads\n",
    "* Carry out a toy scrape of the Study portals website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "from eis.utils.data_processing import *\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "import altair as alt\n",
    "from altair_saver import save\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "\n",
    "#Altair saving code\n",
    "w = make_altair_save()\n",
    "\n",
    "save_dir = f\"{project_dir}/reports/figures/final_report\"\n",
    "\n",
    "def save_altair_(f,n):\n",
    "    save_altair(f,n,w,fig_path=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Various credentials to collect Nesta and Google Big Query data\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "cp = os.environ.get('config_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_name(code_var,lookup):\n",
    "    \n",
    "    return code_var.apply(lambda x: x.lower()).map(lookup)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_codes = pd.read_csv(\n",
    "#     'https://www.eea.europa.eu/data-and-maps/data/waterbase-lakes-4/country-codes-and-abbreviations-32-records/country-codes-and-abbreviations-32-records/at_download/file')\n",
    "\n",
    "#Load country code - name lookup\n",
    "country_codes_lu = pd.read_csv(f\"{data_path}/aux/eu_iso_2_name_lookup.csv\").set_index('Unnamed: 0')['0'].to_dict()\n",
    "\n",
    "country_codes = set(country_codes_lu.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "ind = pd.read_csv(f\"{project_dir}/data/aux/eis_indicator_inventory.csv\",na_values='TBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind.loc[ind['method_type']=='Web'][\n",
    "    ['category','indicator','source','description']].to_csv(f\"{material_outputs}/table_4_web.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = pd.read_excel('https://development-data-hub-s3-public.s3.amazonaws.com/ddhfiles/144635/public_use-talent-migration.xlsx',\n",
    "                  sheet_name='Skill Migration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_codes = country_codes\n",
    "eu_li = li.loc[[x in eu_codes for x in li['country_code']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = ['Artificial Intelligence','Data Science','Natural Language Processing']\n",
    "eu_ai = eu_li.loc[[x in ai for x in eu_li['skill_group_name']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = ['country_name','country_code','net_per_10K_2016','net_per_10K_2017','net_per_10K_2018','net_per_10K_2019']\n",
    "\n",
    "eu_ai_long = eu_ai[rel].melt(id_vars=['country_name','country_code'])\n",
    "\n",
    "eu_ai_long['year'] = [int(x.split('_')[-1]) for x in eu_ai_long['variable']]\n",
    "\n",
    "ai_agg = eu_ai_long.pivot_table(\n",
    "    index=['country_code','country_name'],columns='year',values='value',aggfunc='sum').sort_values(2018,ascending=True)\n",
    "\n",
    "ai_agg = ai_agg.T.rolling(window=2).mean().dropna().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_agg_long = ai_agg.reset_index(drop=False).melt(id_vars=['country_code','country_name'],\n",
    "                                                  var_name='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(ai_agg_long)\n",
    "\n",
    "a = base.mark_point().encode(y=alt.Y('country_name',axis=alt.Axis(grid=True,gridWidth=1),\n",
    "                                    sort=alt.EncodingSortField('value','mean',order='descending')),\n",
    "                                     x=alt.X('value',title='Net flow of talent per 10K members'),\n",
    "                             color='year:N',shape='year:N')\n",
    "b = base.mark_line(strokeDash=[3,1],strokeWidth=1).encode(\n",
    "    y=alt.Y('country_name:N',title=None,sort=alt.EncodingSortField('value','mean',order='descending')),\n",
    "    x='value:Q',detail=alt.Detail('country_name:N'))\n",
    "\n",
    "c = base.mark_rule().transform_calculate(zero='0').encode(x='zero:Q')\n",
    "\n",
    "d = (a+b+c).properties(height=500)\n",
    "\n",
    "save_altair_(d,'fig_8_linkedin')\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meetup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{project_dir}/data/raw/eu_meetup.p\",'rb') as infile:\n",
    "    eu_meetup_groups = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = pd.concat([x['core_groups'] for x in eu_meetup_groups]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some parsing\n",
    "\n",
    "#of years\n",
    "gs['year'] = [datetime.fromtimestamp(np.float(x)/1000).year for x in gs['created']]\n",
    "\n",
    "#Of topics\n",
    "gs['topic_list'] = [literal_eval(x) for x in gs['topics']]\n",
    "gs['topic_kws'] = [[x['urlkey'] for x in el] for el in gs['topic_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tag meetups with AI keywords\n",
    "ai = set(['machine-learning','ai','deep-learning','data-science'])\n",
    "vr = set(['virtual-reality','augmented-reality','vr'])\n",
    "crypto = set(['cryptocurrency','blockchain','bitcoin'])\n",
    "\n",
    "gs['Artificial Intelligence'],gs['Virtual Reality'],gs['Crypto'] = [\n",
    "    [int(len(tech_set & set(kws))>0) for kws in gs['topic_kws']] for tech_set in [ai,vr,crypto]]\n",
    "\n",
    "gs['country_name'] = get_country_name(gs['country'],country_codes_lu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetup_long = pd.crosstab(gs['country_name'],gs['year']).cumsum(axis=1).reset_index(drop=False).melt(id_vars='country_name',value_name='groups')\n",
    "\n",
    "#meetup_long['country_name'] = get_country_name(meetup_long['country'],country_codes_lu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_1 = alt.Chart(meetup_long,width=60,height=70).mark_line().encode(x='year:O',\n",
    "                                                 y='groups',\n",
    "                                         facet=alt.Facet('country_name',columns=8,title='Country',\n",
    "                                                        sort=alt.EncodingSortField('groups','max',\n",
    "                                                                          order='descending')))\n",
    "\n",
    "\n",
    "save_altair_(ch_1,\"fig_9_meetup_trends\")\n",
    "ch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_tech_ev = (100*gs.groupby(['year'])[['Artificial Intelligence','Virtual Reality','Crypto']\n",
    "                    ].mean()).reset_index(drop=False).melt(id_vars='year',var_name='Technology activity')\n",
    "\n",
    "em_tech_geo = gs.groupby(['country_name'])[['Artificial Intelligence','Virtual Reality','Crypto']\n",
    "                    ].sum().apply(lambda x: 100*x/x.sum()).reset_index(drop=False).melt(id_vars='country_name',\n",
    "                                                                                   var_name='Technology',\n",
    "                                                                                       value_name='Share')\n",
    "\n",
    "# name_lookup = {'has_ai':'AI','has_crypto':['Cryptocurrencies', '& Blockchain'],'has_vr':'immersive'}\n",
    "\n",
    "# sort_tech = list(name_lookup.values())\n",
    "\n",
    "# em_tech_geo['tech_clean'] = em_tech_geo['Technology'].map(name_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_3 = alt.Chart(em_tech_ev,width=600).transform_window(\n",
    "    mean_value='mean(value)',groupby=['Technology activity'],frame=[-1,+1]).mark_line().encode(\n",
    "x=alt.X('year:O'),y=alt.Y('mean_value:Q',title='% of activity accounted by year'),color='Technology activity')\n",
    "\n",
    "\n",
    "ch_4 = alt.Chart(em_tech_geo).mark_circle(stroke='black',\n",
    "                                         strokeWidth=1).encode(y=alt.Y('Technology:N',\n",
    "                                                                       sort=['Artificial Intelligence','Crypto',\n",
    "                                                                             'Virtual Reality']),\n",
    "                                         x = alt.X('country_name',sort=alt.EncodingSortField(\n",
    "                                             'Share','sum',order='descending')),\n",
    "                                                               size=alt.Size('Share',legend=None),\n",
    "                                                               color='Share').properties(height=100)\n",
    "\n",
    "ch_5 = alt.vconcat(ch_3,ch_4)\n",
    "\n",
    "save_altair_(ch_5,\"fig_10_meetup_trends\")\n",
    "\n",
    "ch_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google big queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = service_account.Credentials.from_service_account_file(\n",
    "    f\"{project_dir}/gbq_eis_credentials.json\")\n",
    "\n",
    "project_id = 'eis-2-275207'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This query extracts a count of unique year by year of registration and country code\n",
    "#Removing fake accounts\n",
    "q1 = '''SELECT EXTRACT (YEAR FROM created_at), COUNT(id), country_code\n",
    "FROM `ghtorrentmysql1906.MySQL1906.users`\n",
    "WHERE fake = 0 AND deleted = 0\n",
    "GROUP BY country_code, EXTRACT (YEAR FROM created_at)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_reg = pandas_gbq.read_gbq(q1, \n",
    "                project_id='eis-2-275207',\n",
    "                credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_reg.rename(columns={'f0_':'year_created','f1_':'user_count','country_code':'country_code'},\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_wide = github_reg.pivot_table(index='year_created',columns='country_code',\n",
    "                                    values='user_count').fillna(0)\n",
    "top_github_eu = github_wide[[x for x in eu_codes if x in github_wide.columns]].sum().sort_values(ascending=False)\n",
    "\n",
    "top_gh_eu_names = top_github_eu[:7].index\n",
    "\n",
    "eu_totals = pd.concat([github_wide[top_gh_eu_names],\n",
    "           github_wide[[x for x in github_wide.columns if (x in eu_codes) & (x not in top_gh_eu_names)\n",
    "                       ]].sum(\n",
    "               axis=1).rename('other')],\n",
    "         axis=1).cumsum().T\n",
    "\n",
    "#Need this to order the variables\n",
    "eu_totals['order'] = list(range(0,8))\n",
    "eu_totals_long = eu_totals.reset_index(drop=False).melt(id_vars=['index','order'])\n",
    "\n",
    "eu_totals_long['country_name'] = [country_codes_lu[x] if x in country_codes_lu.keys() else 'Other' for x in eu_totals_long['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_g = alt.Chart(eu_totals_long).mark_area(stroke='Grey').encode(\n",
    "    x=alt.X('year_created:O',title='Year'),\n",
    "    y=alt.Y('value',title='Registered members (cumulative)'),\n",
    "    color=alt.Color('country_name',title='Country',\n",
    "                    sort=list(eu_totals.index)[::-1],scale=alt.Scale(scheme='Accent')),order='order').properties(\n",
    "    width=400)\n",
    "\n",
    "save_altair_(ch_g,'fig_11_github')\n",
    "\n",
    "ch_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_count_df = top_github_eu.reset_index(drop=False).rename(columns={0:'Registered users'})\n",
    "\n",
    "github_count_df['country_name'] = github_count_df['country_code'].map(country_codes_lu)\n",
    "\n",
    "ch_g_us = alt.Chart(github_count_df,height=200).mark_bar().encode(x=alt.X('country_name',sort=alt.EncodingSortField('count',\n",
    "                                                    order='descending')),\n",
    "                                             y='Registered users').properties(width=550)\n",
    "\n",
    "save_altair_(ch_g_us,'fig_12_github_country_count')\n",
    "ch_g_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_github_eu.sum()/github_wide.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyq_all = '''SELECT COUNT(*), country_code\n",
    "FROM `the-psf.pypi.file_downloads` \n",
    "WHERE DATE(timestamp) = \"{}\" \n",
    "GROUP BY country_code'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyq_ml = '''SELECT COUNT(*), country_code\n",
    "FROM `the-psf.pypi.file_downloads` \n",
    "WHERE file.project in ('tensorflow','keras','pytorch','sklearn') AND DATE(timestamp) = \"{}\" \n",
    "GROUP BY country_code'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_all = [pandas_gbq.read_gbq(pyq_all.format(f'2020-04-0{str(n)}'), \n",
    "                project_id='eis-2-275207',\n",
    "                credentials=creds) for n in np.arange(1,7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_ml = [pandas_gbq.read_gbq(pyq_ml.format(f'2020-04-0{str(n)}'), \n",
    "                project_id='eis-2-275207',\n",
    "                credentials=creds) for n in np.arange(1,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_downloads = pd.concat([\n",
    "    pd.concat(df).groupby('country_code')['f0_'].mean().rename(name) for df,name in \n",
    "    zip([py_all,py_ml],['all_files','ml_packages'])],axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_downloads.index = [x.lower() for x in py_downloads.index]\n",
    "\n",
    "py_downloads.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro_py = py_downloads.loc[py_downloads['index'].isin(country_codes)].sort_values('all_files',ascending=False)\n",
    "\n",
    "euro_py['country_name'] = euro_py['index'].map(country_codes_lu)\n",
    "euro_py = euro_py.drop('index',axis=1).set_index('country_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro_py_long = (100*euro_py.apply(lambda x: x/x.sum())).reset_index(drop=False).melt(id_vars='country_name',\n",
    "                                                                      var_name='download_type',value_name=\n",
    "                                                                                     'download_share')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro_py_long['download_type'] = ['All' if x=='all_files' else \"Machine Learning packages\" for x in euro_py_long['download_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(euro_py_long)\n",
    "\n",
    "x_pos = alt.X('country_name',title='Country',\n",
    "              sort=alt.EncodingSortField(field='download_share',op='sum',order='descending'))\n",
    "y_pos = alt.Y('download_share',title='Share of EU downloads')\n",
    "\n",
    "p = base.mark_point(filled=True,size=50,stroke='black',strokeWidth=1).encode(x=x_pos,y=y_pos,color='download_type',\n",
    "                             shape=alt.Shape('download_type',title='Type of download'))\n",
    "\n",
    "l = base.mark_line(strokeDash=[1,2]).encode(x=x_pos,y=y_pos,detail='country_name')\n",
    "\n",
    "f = (p+l).properties(width=500,height=200)\n",
    "\n",
    "save_altair_(f,\"fig_13_pydownloads\")\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second one extracts count of activity in a location by year\n",
    "q2 = '''SELECT EXTRACT (YEAR FROM creation_date), COUNT(id), location\n",
    "FROM `bigquery-public-data.stackoverflow.users` \n",
    "GROUP BY location, EXTRACT (YEAR FROM creation_date) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackover = pandas_gbq.read_gbq(q2, \n",
    "                project_id='eis-2-275207',\n",
    "                credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stack_locs = stackover.groupby('location')['f1_'].sum().sort_values(\n",
    "    ascending=False)[:20].reset_index(drop=False).rename(columns={'f1_':'users'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = alt.Chart(top_stack_locs).mark_bar().encode(y=\n",
    "                                            alt.Y('location:O',title='Location',\n",
    "                                                  sort=alt.EncodingSortField('users',order='descending')),\n",
    "                                                  x='users:Q').properties(width=200,height=250)\n",
    "\n",
    "\n",
    "save_altair_(stack,'fig_14_stackover')\n",
    "\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StudyPortals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spath = f\"{project_dir}/data/raw/studyportals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(spath)==False:\n",
    "    os.mkdir(spath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = requests.get(\"https://github.com/nestauk/eis/blob/3_studydata/data/raw/courses.zip?raw=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZipFile(BytesIO(file.content)).extractall(spath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(spath+'/courses/108/bachelor/108-bachelor-1000.json','r') as infile:\n",
    "    test = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_courses = []\n",
    "\n",
    "for file in os.listdir(spath+'/courses'):\n",
    "    if not any(ext in file for ext in ['json','txt']):\n",
    "        course_types = os.listdir(spath+f'/courses/{file}')\n",
    "        for level in course_types:\n",
    "            jsons = os.listdir(spath+f'/courses/{file}/{level}')\n",
    "            for j in jsons:\n",
    "                with open(spath+f'/courses/{file}/{level}/{j}','r') as infile:\n",
    "                    courses = json.load(infile)\n",
    "                    courses_df = pd.DataFrame(courses)\n",
    "                    comp_courses.append(courses_df)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df = pd.concat(comp_courses).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df['venues_n'] = sp_df['venues'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df['country'] = [[x['country'] for x in vens][0] for vens in sp_df['venues']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(sp_df['id'])),'  ',len(set(sp_df['country'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to focus on the EU\n",
    "\n",
    "with open(f\"{data_path}/aux/eu_codes_names.txt\",'r') as infile:\n",
    "    eu_27_other_codes = infile.read().split(', ')\n",
    "    \n",
    "eu_27_names = [x.split(': ')[1].lower().split(',')[0] for x in eu_27_other_codes]+['liechtenstein','macedonia (fyrom)']\n",
    "\n",
    "sp_df['country_lower'] = sp_df['country'].apply(lambda x: x.lower())\n",
    "\n",
    "sp_df_eu = sp_df.loc[sp_df['country_lower'].isin(eu_27_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sp_df_eu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df_eu['country_lower'].value_counts(normalize=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*sp_df_eu['level'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do we select\n",
    "#Note that there are some duplicated courses because they are assigned multiple disciplines\n",
    "country_discipline_activity = sp_df_eu.groupby(['country','level','discipline_title']).size().reset_index(name='course_n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "country_chart = (alt.Chart(country_discipline_activity)\n",
    "                 .mark_point(filled=False,shape='square')\n",
    "                 .encode(x=alt.Y('discipline_title',title='Discipline',\n",
    "                                 sort=alt.EncodingSortField('course_n','sum',order='descending')),\n",
    "                         y=alt.X('country',title='Country',\n",
    "                                 sort=alt.EncodingSortField('course_n','sum',order='descending')),\n",
    "                         size=alt.Size('course_n',title='Number of courses'),\n",
    "                         color=alt.Color('level',title='Level',\n",
    "                                         scale=alt.Scale(scheme='Dark2')))).properties(width=275)\n",
    "\n",
    "country_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_discipline = country_discipline_activity.groupby(\n",
    "    ['country','discipline_title'])['course_n'].sum().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_levels = (alt.Chart(country_discipline)\n",
    "                 .mark_bar()\n",
    "                 .encode(y=alt.X('country',sort=alt.EncodingSortField('course_n',order='descending')),\n",
    "                         x=alt.X('course_n'),\n",
    "                         color=alt.Color('discipline_title',\n",
    "                                         sort=alt.EncodingSortField('course_n',order='descending'),\n",
    "                                         scale=alt.Scale(scheme='category20')),\n",
    "                        order=alt.Order('course_n',sort='descending'))).properties(height=450,width=200)\n",
    "\n",
    "save_altair_(country_levels,\"fig_15_country_courses\")\n",
    "\n",
    "country_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_level = sp_df_eu.groupby(['discipline_title','level']).size().reset_index(name='course_n')\n",
    "\n",
    "d = (alt.Chart(discipline_level)\n",
    "     .mark_bar()\n",
    "     .encode(y=alt.Y('discipline_title',sort=alt.EncodingSortField('course_n',order='descending'),title='Discipline'),\n",
    "             x=alt.X('course_n',title='Number of courses'),\n",
    "             color=alt.Color('level',title='Level',sort=alt.EncodingSortField('course_n')))).properties(width=250)\n",
    "\n",
    "save_altair_(d,\"fig_16_discipline_level\")\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all the data for the analytical synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetup_long = pd.crosstab(\n",
    "    gs['country'],gs['year']).reset_index(drop=False).melt(id_vars=['country'],\n",
    "                                                           value_name='tech_meetups')\n",
    "github_long = github_wide.reset_index(drop=False).melt(\n",
    "    id_vars='year_created',value_name='github_users').rename(\n",
    "    columns={'year_created':'year','country_code':'country'})\n",
    "\n",
    "python_long = py_downloads.loc[py_downloads['index'].isin(country_codes)][['index','all_files']].rename(columns={\n",
    "    'index':'country','all_files':'python_downloads'}).assign(year=2018)\n",
    "\n",
    "out = pd.concat([x.melt(id_vars=['country','year']) for x in [meetup_long,github_long,python_long]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(f\"{project_dir}/data/processed/web_indicators.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
