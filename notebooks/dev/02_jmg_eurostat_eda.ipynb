{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eurostat EDA\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Load indicator spreadsheet and output a couple of tables/figures for the report\n",
    "    1. Table with key dimensions\n",
    "    2. figure comparing various dimensions of the indicators\n",
    "    3. Table with Eurostat indicators\n",
    "2. Read and process Eurostat data\n",
    "    1. Write functions to filter tables at the levels we need\n",
    "    2. Produce coverage figures (country x year x indicator)\n",
    "    3. Analyse geography and trends\n",
    "    4. Analyse correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "import seaborn as sn\n",
    "import yaml\n",
    "from scipy.stats import zscore\n",
    "from eis.utils.data_processing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_outputs = f\"{project_dir}/reports/figures/exploratory_paper\"\n",
    "\n",
    "plt.style.use('seaborn-muted')\n",
    "plt.rc('font', size=12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Indicator spreadsheet task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "ind = pd.read_csv(f\"{project_dir}/data/aux/eis_indicator_inventory.csv\",na_values='TBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns = ['category','method_type','temporal_coverage','geographical_coverage',\n",
    "             'geographical_resolution','trustworthiness','complexity']\n",
    "table = pd.DataFrame({'dimension':my_columns,'observations':['']*len(my_columns)})\n",
    "table.to_csv(f'{material_outputs}/table_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of indicator dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.crosstab(ind['method_type'],ind['geographical_resolution'],normalize=0).plot.barh(figsize=(6,4))\n",
    "ax.set_xlabel('% of indicators in category')\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('fig_1_geo_resolution.pdf',material_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.crosstab(ind['method_type'],ind['sectoral_resolution'],normalize=0).plot.barh(figsize=(8,4))\n",
    "ax.set_xlabel('% of indicators in category')\n",
    "ax.legend(bbox_to_anchor=(1.1,1),title='Sectoral resolution')\n",
    "\n",
    "save_fig('fig_2_sectoral_resolution.pdf',material_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_last_year = ind.assign(last_year = lambda x: [int(v.split(',')[1]) if ',' in v else int(v) for v in x['temporal_coverage']])\n",
    "pd.DataFrame(ind_last_year.groupby('method_type')['last_year'].mean()).T.to_csv(f\"{material_outputs}/table_2_mean_last_year.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ind.groupby('method_type')[['trustworthiness','complexity']].mean().plot.barh()\n",
    "ax.set_xlabel('Average score')\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_3_complexity_trustworthiness.pdf',material_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eurostat indicator analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \n",
    "\n",
    "sn.set_palette('Purples_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind.loc[ind['source']=='Eurostat'][['category','indicator','description']].sort_values(\n",
    "    'category').to_csv(f\"{material_outputs}/table_3_es_indicators.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the indicators\n",
    "\n",
    "indicator_location = f\"{project_dir}/data/raw/eurostat/selected_tables\"\n",
    "\n",
    "indicator_store = {}\n",
    "\n",
    "for x in os.listdir(indicator_location):\n",
    "    \n",
    "    if 'csv' in x:\n",
    "        indicator_store[x.split('.')[0]] = pd.read_csv(f\"{indicator_location}/{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{project_dir}/data/aux/eis_filters.yaml','r') as infile:\n",
    "    all_filters = yaml.safe_load(infile)\n",
    "    \n",
    "with open(f'{project_dir}/data/aux/eurostat_clean_names.json','r') as infile:\n",
    "    es_clean_names = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,filter_dict,var_name,make_concise=True):\n",
    "    '''\n",
    "    Filters a df with the keys and values of a filter_dict\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe) is a dataframe\n",
    "        filter_dict (dict) is a dict where the keys are filter variables and the values are filter values (lists)\n",
    "        make_concise (str) only returns country, year and variable\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "\n",
    "    \n",
    "    for k,v in filter_dict.items():\n",
    "        df_2 = df_2.loc[[x in v for x in df_2[k]]]\n",
    "    \n",
    "    if make_concise==True:\n",
    "        return(df_2.reset_index(drop=True)[['geo\\\\time','time',var_name]])\n",
    "    else:\n",
    "        return(df_2.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_category = [\"supply\",\"supply\",\"supply\",\"industrial_skill_base\",\n",
    "                      \"demand\",\"demand\",\"demand\",\"supply\",\n",
    "                      \"industrial_skill_base\",\"industrial_skill_base\",\n",
    "                      \"industrial_skill_base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we create a dict where every element is a df\n",
    "filtered_dfs = {}\n",
    "\n",
    "for l in es_clean_names.keys():\n",
    "    \n",
    "    df = filter_df(indicator_store[l],\n",
    "          all_filters[l],l)\n",
    "    \n",
    "    df_pivoted = df.pivot_table(\n",
    "        index='geo\\\\time',columns='time',values=l,aggfunc='sum').replace(0,np.nan)\n",
    "    \n",
    "    #Drop EU aggregates and sort by the last year\n",
    "    df_processed = df_pivoted.drop(\n",
    "        [x for x in df_pivoted.index if any(v in x for v in ['EU','EA'])])\n",
    "    \n",
    "    sort_countries = df_processed.mean(axis=1).sort_values(ascending=False).index\n",
    "    \n",
    "    filtered_dfs[l] = df_processed.loc[sort_countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot(rows,cols,dfs,titles,figsize=(12,16)):\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=figsize,nrows=rows,ncols=cols)\n",
    "    \n",
    "    row = 0\n",
    "    col = 0\n",
    "    \n",
    "    for n,df in enumerate(dfs):\n",
    "        sn.heatmap(dfs[n],ax=ax[row,col],cmap='Purples')\n",
    "        ax[row,col].set_title('\\n ('.join(titles[n].split('(')))\n",
    "        ax[row,col].set_yticks([x+0.5 for x in np.arange(len(dfs[n]))])\n",
    "        ax[row,col].set_yticklabels(dfs[n].index,rotation=0,size=10)\n",
    "        \n",
    "        ax[row,col].set_ylabel('')\n",
    "\n",
    "        col+=1\n",
    "        if col>1:\n",
    "            col=0\n",
    "            row+=1\n",
    "            \n",
    "    if len(dfs)<rows*cols:\n",
    "        ax[row,1].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_indicators = [[x for x,y in zip(filtered_dfs.keys(),framework_category) if y in val] for val in \n",
    "    ['supply','demand','industrial_skill_base']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=0\n",
    "\n",
    "for n,group in enumerate(classified_indicators):\n",
    "    \n",
    "    dfs = [filtered_dfs[x] for x in group]\n",
    "    names = [es_clean_names[x] for x in group]\n",
    "    \n",
    "    multiplot(2,2,dfs,names,figsize=(10,15))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_fig(f\"fig_4_{n}_time_country_coverage.pdf\",material_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to merge all variables on country and year\n",
    "df_long_list = []\n",
    "\n",
    "#For each name and df in the df container\n",
    "for k,v in filtered_dfs.items():\n",
    "    d_2 = v.copy()\n",
    "    \n",
    "    d_2 = d_2.reset_index(drop=False)\n",
    "    \n",
    "    #Melt\n",
    "    d_long = d_2.melt(id_vars=['geo\\\\time'],var_name='year',value_name=k)\n",
    "    \n",
    "    #Append\n",
    "    df_long_list.append(d_long.set_index(['geo\\\\time','year']))\n",
    "\n",
    "es_merged = pd.concat(df_long_list,axis=1).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calculates correlations on a year basis\n",
    "geo_corrs = {}\n",
    "count_corrs = {}\n",
    "\n",
    "for y in np.arange(2010,2020):\n",
    "    \n",
    "    in_year = es_merged.loc[es_merged['year']==y]\n",
    "    \n",
    "    in_year_sel = in_year.drop('year',axis=1).set_index('geo\\\\time')\n",
    "    \n",
    "    geo_corrs[y] = in_year_sel.corr()\n",
    "    \n",
    "    count_corrs[y] = in_year_sel.apply(lambda x: zscore(x,nan_policy='omit')).T.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an average correlation by variable over year\n",
    "\n",
    "indicator_focus = [x for x in filtered_dfs.keys() if x != 'isoc_ske_fct']\n",
    "\n",
    "indicator_corr = pd.DataFrame(index=indicator_focus,columns=indicator_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in indicator_focus:\n",
    "    \n",
    "    for w in indicator_focus:\n",
    "        \n",
    "        if v == w:\n",
    "            indicator_corr.loc[v,w]=1\n",
    "            \n",
    "        else:\n",
    "            pair_corrs = []\n",
    "            for df in geo_corrs.values():\n",
    "                pair_corrs.append(df.loc[v,w].astype(float))\n",
    "                indicator_corr.loc[v,w] = np.float(np.mean([x for x in pair_corrs if pd.isnull(x)==False]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_corr = indicator_corr[indicator_corr.columns].astype(float) \n",
    "\n",
    "indicator_corr.columns,indicator_corr.index = [[es_clean_names[x] for x in val] for val in \n",
    "                                               [indicator_corr.columns,indicator_corr.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.clustermap(indicator_corr,cmap='coolwarm')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_5_indicator_correlation.pdf',material_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt = pd.concat([dfs.reset_index(drop=False).melt(id_vars=['geo\\\\time','year']) for \n",
    "                  dfs in df_long_list]).rename(columns={'geo\\\\time':'country'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt.to_csv(f\"{project_dir}/data/interim/official_indicators.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
